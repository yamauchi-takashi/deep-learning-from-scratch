{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章 誤差逆伝播法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 逆伝播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 加算ノードの逆伝播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z = x + y \\Longrightarrow \\frac{\\partial z}{\\partial x} = 1, \\hspace{2mm} \\frac{\\partial z}{\\partial y} = 1\n",
    "$$\n",
    "従って、逆伝播は$\\displaystyle \\frac{\\partial L}{\\partial z}$の値がそのまま伝わる。\n",
    "\n",
    "<img src=\"./figures/Fig05-9.jpg\" width=700 alt=\"Fig05-9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 乗算ノードの逆伝播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z = x y \\Longrightarrow \\frac{\\partial z}{\\partial x} = y, \\hspace{2mm} \\frac{\\partial z}{\\partial y} = x\n",
    "$$\n",
    "\n",
    "<img src=\"./figures/Fig05-12.jpg\" width=700 alt=\"Fig05-12\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 単純なレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 乗算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y                \n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リンゴの例\n",
    "\n",
    "<img src=\"./figures/Fig05-16.jpg\" width=700 alt=\"Fig05-16\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 220\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dTax: 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 加算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リンゴとみかんの例\n",
    "\n",
    "<img src=\"./figures/Fig05-17.jpg\" width=700 alt=\"Fig05-17\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 ReLU (Rectified Linear Unit) レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = \\begin{cases} x & (x > 0) \\\\ 0 & (x \\le 0) \\end{cases} \\Longrightarrow \\frac{\\partial y}{\\partial x} = \\begin{cases} 1 & (x > 0) \\\\ 0 & (x \\le 0) \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoidレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = \\frac{1}{1 + \\exp (-x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接微分した場合\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = - \\frac{1}{(1 + \\exp (-x) )^2} (- \\exp (-x)) = y^2 \\exp (-x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算グラフの場合\n",
    "\n",
    "<img src=\"./figures/Fig05-20.jpg\" width=700 alt=\"Fig05-20\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「$/$」ノード $\\hspace{2mm} \\displaystyle y = \\frac{1}{x} \\Longrightarrow \\frac{\\partial y}{\\partial x} = - \\frac{1}{x^2} = - y^2$\n",
    "\n",
    "「$+$」ノード $\\hspace{2mm}$値はそのまま\n",
    "\n",
    "「$\\exp$」ノード $\\hspace{2mm} \\displaystyle y = \\exp (x) \\Longrightarrow \\frac{\\partial y}{\\partial x} = \\exp (x)$\n",
    "\n",
    "「$\\times$」ノード $\\hspace{2mm} \\displaystyle y = -1 \\cdot x \\Longrightarrow \\frac{\\partial y}{\\partial x} = -1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = y^2 \\exp (-x) = \\frac{1}{1 + \\exp (-x)} \\frac{\\exp (-x)}{1 + \\exp (-x)}= y (1-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine / Softmax レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 Affineレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 入力が1つ (行ベクトル) の場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/Fig05-25.jpg\" width=700 alt=\"Fig05-25\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 数式の導出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\frac{\\partial L}{\\partial \\boldsymbol{x}} = \\frac{\\partial L}{\\partial \\boldsymbol{y}} \\cdot W^T$であること\n",
    "\n",
    "$\\boldsymbol{x} = (x_i), \\boldsymbol{y} = (y_i), W = (a_{ij})$とする。$x_i W_{ij} = y_j$とすると\n",
    "$$\n",
    "\\left( \\frac{\\partial L}{\\partial \\boldsymbol{x}} \\right)_k = \\frac{\\partial L}{\\partial x_k} = \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial x_k} = \\frac{\\partial L}{\\partial y_i} \\frac{\\partial}{\\partial x_k} (x_j W_{ji}) = \\frac{\\partial L}{\\partial y_i} \\delta_{kj} W_{ji} = \\frac{\\partial L}{\\partial y_i} W_{ki} = \\left( \\frac{\\partial L}{\\partial \\boldsymbol{y}} \\cdot W^T \\right)_k\n",
    "$$\n",
    "\n",
    "$\\displaystyle \\frac{\\partial L}{\\partial W} = \\boldsymbol{x}^T \\cdot \\frac{\\partial L}{\\partial \\boldsymbol{y}}$であること\n",
    "\n",
    "$$\n",
    "\\left( \\frac{\\partial L}{\\partial W} \\right)_{kl} = \\frac{\\partial L}{\\partial W_{kl}} = \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial W_{kl}} =\\frac{\\partial L}{\\partial y_i} \\frac{\\partial}{\\partial W_{kl}} (x_j W_{ji}) = \\frac{\\partial L}{\\partial y_i} x_j \\delta_{kj} \\delta_{li} = \\frac{\\partial L}{\\partial y_l} x_k = x_k \\frac{\\partial L}{\\partial y_l} = \\left( \\boldsymbol{x}^T \\cdot \\frac{\\partial L}{\\partial \\boldsymbol{y}} \\right)_{kl}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### バッチ処理の場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/Fig05-27.jpg\" width=700 alt=\"Fig05-27\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 数式の導出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2]と[3]が個々のサンプルの結果の和であることから導く。\n",
    "\n",
    "入力、出力ともに行ベクトルなので、バッチ処理の場合\n",
    "$$\n",
    "X = \\begin{pmatrix} \\boldsymbol{x}_1 \\\\ \\boldsymbol{x}_2 \\end{pmatrix}, \\hspace{2mm} \\frac{\\partial L}{\\partial Y} = \\begin{pmatrix} d\\boldsymbol{y}_1 \\\\ d\\boldsymbol{y}_2 \\end{pmatrix}\n",
    "$$\n",
    "と置く。\n",
    "\n",
    "まず、[2]の場合、\n",
    "$$\n",
    "\\boldsymbol{x}_1^T d\\boldsymbol{y}_1 + \\boldsymbol{x}_2^T d\\boldsymbol{y}_2 = \\begin{pmatrix} \\boldsymbol{x}_1^T & \\boldsymbol{x}_2^T \\end{pmatrix} \\begin{pmatrix} d\\boldsymbol{y}_1 \\\\ d\\boldsymbol{y}_2 \\end{pmatrix} = X^T \\frac{\\partial L}{\\partial Y}\n",
    "$$\n",
    "\n",
    "次に[3]の場合\n",
    "\n",
    "$$\n",
    "d\\boldsymbol{y}_1 + d\\boldsymbol{y}_2\n",
    "$$\n",
    "\n",
    "は$\\displaystyle \\frac{\\partial L}{\\partial Y}$の最初の軸 (第0軸) に関する和となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # テンソル対応\n",
    "        self.x = x\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Lossレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソフトマックス関数\n",
    "$$\n",
    "S = \\sum_{i=1}^n \\exp(a_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_k = \\frac{\\exp (a_k)}{\\displaystyle \\sum_{i=1}^n \\exp(a_i)} = \\frac{\\exp (a_k)}{S}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy Errorレイヤ\n",
    "\n",
    "$$\n",
    "L = - \\sum_{k} t_k \\log y_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy Errorレイヤの逆伝播\n",
    "\n",
    "<img src=\"./figures/FigA-4.jpg\" width=700 alt=\"FigA-4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分岐レイヤの逆伝播\n",
    "\n",
    "$x \\rightarrow u, v \\rightarrow L$で$u = x, v = x, L (u, v)$とすると\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial u} \\frac{\\partial u}{\\partial x} + \\frac{\\partial L}{\\partial v} \\frac{\\partial v}{\\partial x} = \\frac{\\partial L}{\\partial u} + \\frac{\\partial L}{\\partial v}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmaxレイヤの逆伝播\n",
    "\n",
    "<img src=\"./figures/FigA-5.jpg\" width=700 alt=\"FigA-5\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "「$\\times$」ノード\n",
    "$$\n",
    "- \\frac{t_1}{y_1} \\exp(a_1) = - t_1 \\frac{S}{\\exp(a_1)} \\exp(a_1) = - t_1 S\n",
    "$$\n",
    "$$\n",
    "- \\frac{t_1}{y_1} \\frac{1}{S} = - t_1 \\frac{S}{\\exp(a_1)} \\frac{1}{S} = - \\frac{t_1}{\\exp (a_1)}\n",
    "$$\n",
    "\n",
    "「逆数 ($/$)」と分岐のノード\n",
    "$$\n",
    "y = \\frac{1}{x} \\rightarrow \\frac{\\partial y}{\\partial x} = - \\frac{1}{x^2}\n",
    "$$\n",
    "$$\n",
    "- (t_1 S + t_2 S + t_3 S) \\frac{-1}{S^2} = \\frac{1}{S} (t_1 + t_2 + t_3)\n",
    "$$\n",
    "one-hot表現なので$t_1, t_2, t_3$のどれかが1でその他は0なので$t_1 + t_2 + t_3 = 1$。よって\n",
    "$$\n",
    "\\frac{1}{S}\n",
    "$$\n",
    "\n",
    "「$\\exp$」ノード\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} \\exp (x) = \\exp (x)\n",
    "$$\n",
    "$$\n",
    "\\left( \\frac{1}{S} - \\frac{t_1}{\\exp (a_1)} \\right) \\exp (a_1) = \\frac{\\exp (a_1)}{S} - t_1 = y_1 - t_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 誤差逆伝播法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2 誤差逆伝播法に対応したニューラルネットワークの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions.py\n",
    "\n",
    "layers.py\n",
    "\n",
    "two_layer_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_neuralnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11661666666666666 0.1169\n",
      "0.90535 0.9116\n",
      "0.922 0.9258\n",
      "0.9344333333333333 0.9357\n",
      "0.94375 0.9421\n",
      "0.9505 0.9471\n",
      "0.9562666666666667 0.9518\n",
      "0.9610666666666666 0.9588\n",
      "0.9641833333333333 0.9602\n",
      "0.9675166666666667 0.9611\n",
      "0.9695166666666667 0.9637\n",
      "0.9723666666666667 0.9667\n",
      "0.9713833333333334 0.9665\n",
      "0.9749666666666666 0.9691\n",
      "0.9768 0.9693\n",
      "0.9786333333333334 0.9717\n",
      "0.97865 0.9716\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch05.two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
